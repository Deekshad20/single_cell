{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import nferx_py.fn as nf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from multiprocessing import pool\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from nltk.stem import SnowballStemmer\n",
    "from gensim.utils import simple_preprocess\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn import preprocessing\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics import auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nf.authenticate('nfer','6eaa1d27bfa0639f2712191fa55df872')\n",
    "nf.modify_defaults('server', 'preview')\n",
    "nf.modify_defaults('api_server', 'preview')\n",
    "AUTH = nf.AUTH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### notes\n",
    "1. Get local scores for all gene/cell combinations from single cell api for Tabula muris (study 2) and mouse cell atlas (study 15)\n",
    "2. Do \"intra-study\" testing: for each Tabula Muris cluster, (1) identify the subset of genes to consider (e.g. top 0.05% by cohen's D vs. all other cells), (2) calculate cosine similarity between mean CP10K vector of this gene subset and the literature score vectors for all Tabula Muris cell types with this same gene subset. This would give us a ranked list of literature-derived labels for each cluster. Then repeat this with Mouse Cell Atlas. ---- Cosine sim did not work out---- Switched to calculating vector norms of literature scores of top genes from selected from cluster A, accross all clusters \n",
    "\n",
    "- For each cluster to label C:\n",
    "    1. Select the top n genes by Cohen’s D (e.g. top 5, 10, 25)\n",
    "    2. From API, get the fraction of cells in cluster C which express each gene\n",
    "    3. From literature reference table, get the local scores between each reference cell type and the top n genes\n",
    "    4. Multiply “Fraction cells expressing in cluster C” column by each “Ref_CellTypeX_LocalScores” column to get a set of “literature encoded expression vectors” for each reference cell type\n",
    "    5. For each “Lit Encoded Vector” column, calculate L0, L1, L2, and L-inf norms\n",
    "        L0: number of non-zero elements\n",
    "        L1: sum of all values\n",
    "        L2: square root of sum of squares\n",
    "        L-inf: max value\n",
    "    Output: four tables of Clusters To Label (rows) * Reference Cell Types \n",
    "    Table 1 = L0 norm, Table 2 = L1 norm, Table 3 = L2 norm, Table 4 = L-inf norm\n",
    "    \n",
    "-----------\n",
    "Update: 05/15\n",
    "\n",
    "use global scores\n",
    "\n",
    "Few steps to increase AUC: remove non-annotated clusters; try combining global /local\n",
    "-------------step 4 yet to be done------------------------\n",
    "4. Do \"inter-study\" testing: for each Tabula Muris cluster, (1) identify the subset of genes to consider (e.g. top 0.05% by cohen's D vs. all other cells), (2) calculate cosine similarity between mean CP10K vector of this gene subset and the literature score vectors for all Mouse Cell Atlas cell types with this same gene subset. Then repeat this to label all Mouse Cell Atlas clusters using the literature vectors from the Tabula Muris dataset. This will require some handling of synonyms/cell \"families\" which I provided to Katie for this analysis ... but we could go over that later since step 2 alone will take some time to set up and optimize I think"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POST to single cell api to get the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://pre-staging.nferx.com/singlecellapi/study2/summary?'\n",
    "headers = {'content-type' : 'application/json'}\n",
    "res = requests.post(url = URL, params = {'page': 1, 'rows': 3412880}, auth = AUTH)\n",
    "data_all = pd.DataFrame(res.json()['result']['data'])\n",
    "# d.drop(columns=['localScoreCellType', 'localScoreTissueType', 'greaterThanZero'], inplace = True)\n",
    "data_all_backup = data_all.copy(deep = True)\n",
    "# data_all.drop(columns=['localScoreTissueType', 'greaterThanZero'], inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### litreature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = data_all.loc[~(data_all.cellType == 'Not Annotated'),].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Literature_model():\n",
    "\n",
    "    def __init__(self, score_mat_normalized, score_mat_true,\n",
    "                 data = data_all, savepath = './data/results/global_score/', use_score = ''):\n",
    "\n",
    "        self.gene_cluster_score_norm = score_mat_normalized\n",
    "        self.gene_cluster_score_true = score_mat_true\n",
    "        self.savepath = savepath\n",
    "        self.dfs_all = dict() \n",
    "        \n",
    "    def get_top_genes_vector(self, df, top_n = None, quantile = 0.995, \n",
    "                             cols = ['cohenD', 'mean', 'greaterThanZero']):\n",
    "        \n",
    "        #matrix 1: top-genes means (shape: 1*N)\n",
    "        df = df.set_index('geneName')\n",
    "        if top_n:\n",
    "            return df.sort_values(by = 'cohenD', ascending = False)[cols].iloc[0:top_n]\n",
    "\n",
    "        cutoff = df.cohenD.quantile([0.995]).values[0]\n",
    "        top_genes_vec = df.loc[df.cohenD >= cutoff, cols]\n",
    "\n",
    "\n",
    "        return top_genes_vec\n",
    "    \n",
    "    def get_measure_matrix(self, top_genes_subset, metric = 'cosine'):\n",
    "        cluster = top_genes_subset.cluster[0]\n",
    "        #matrix 1: top-genes means (shape: 1*N)\n",
    "        top_genes_mean = top_genes_subset.loc[:, ['geneName', 'mean']].set_index('geneName').transpose()\n",
    "        #matrix 2: top-genes(N) vs all - clusters'(M) local score (shape: N*M)\n",
    "        gene_local_score_subset = self.gene_cluster_score_norm.loc[top_genes_mean.columns,:]\n",
    "        lit_encoded_subset_true = self.gene_cluster_score_true.loc[top_genes_mean.columns,:]\n",
    "        #get literature vector\n",
    "        lit_encoded_subset = gene_local_score_subset.multiply(top_genes_subset.loc[:, 'greaterThanZero'].values, \n",
    "                                                                  axis = 0)\n",
    "\n",
    "        #apply norms\n",
    "\n",
    "        if metric == 'norm-all':\n",
    "            score0 = (lit_encoded_subset_true != 0).sum()\n",
    "#             print(score0.shape, score0)\n",
    "            score1 = lit_encoded_subset.sum()\n",
    "            score2 = lit_encoded_subset.pow(2).sum().pow(1/2)\n",
    "            scoreINF = lit_encoded_subset.max()\n",
    "\n",
    "            return [score0, score1, score2, scoreINF]\n",
    "\n",
    "        elif metric == 'cosine':\n",
    "            assert top_genes_mean.shape[1] == lit_encoded_subset.shape[0]\n",
    "            score = cosine_similarity(top_genes_mean.to_numpy(), lit_encoded_subset.transpose())\n",
    "            score = pd.Series(score.reshape(-1), index = lit_encoded_subset.columns)\n",
    "        elif metric == 'l0':\n",
    "            score = (lit_encoded_subset != 0).sum()\n",
    "        elif metric == 'l1':\n",
    "            score = lit_encoded_subset.sum()\n",
    "        elif metric == 'l2':\n",
    "            score = lit_encoded_subset.pow(2).sum().pow(1/2)\n",
    "        elif metric == 'inf':\n",
    "            score = lit_encoded_subset.max()\n",
    "#         elif metric == 'euc':\n",
    "#             score = distance.euclidean(top_genes_mean.to_numpy(), lit_encoded_subset.transpose())\n",
    "#             score = pd.Series(score.reshape(-1), index = lit_encoded_subset.columns)\n",
    "        return score\n",
    "    \n",
    "    \n",
    "    #top n genes; all norrms\n",
    "    def use_measure(self, n_genes = [5, 10, 25], metric = 'norm-all', \n",
    "                    save = False):\n",
    "        for n in n_genes:\n",
    "            print('Processing : %d genes'%n)\n",
    "            cluster_mean_vectors = data_all.groupby('cluster').apply(lambda x:\n",
    "                                        self.get_top_genes_vector(x, top_n = n))\n",
    "            cluster_mean_vectors.loc[:, 'greaterThanZero'] = cluster_mean_vectors.loc[:, 'greaterThanZero']/100\n",
    "            self.cluster_mean_vectors = cluster_mean_vectors.reset_index()\n",
    "            res = self.cluster_mean_vectors.groupby('cluster').apply(lambda x: self.get_measure_matrix(x, metric))\n",
    "            if metric == 'norm-all':\n",
    "                norms = ['L0', 'L1', 'L2', 'Linf']\n",
    "                for i, norm in enumerate(norms):\n",
    "                    name = '{}_{:02}'.format(norm, n)\n",
    "                    df = res.apply(lambda x: x[i])\n",
    "                    self.dfs_all[name] = df\n",
    "                    if save:\n",
    "                        df.index = df.index + '_TRUE'\n",
    "                        df.to_csv(self.savepath+'{}_{:02}_genes.csv'.format(norm, n))\n",
    "            else:\n",
    "                name = '{}_{:02}'.format(metric, n)\n",
    "                self.dfs_all[name] = res\n",
    "                if save:\n",
    "                    res.index = res.index + '_TRUE'\n",
    "                    res.to_csv(self.savepath+'{}_{:02}_genes.csv'.format(metric, n))\n",
    "        return res\n",
    "\n",
    "    \n",
    "    def run(self, metric = ['norm-all']):\n",
    "        \n",
    "        _ = list(map(lambda x: self.use_measure(metric = x), metric))\n",
    "        self.get_ranks()\n",
    "        \n",
    "    def get_ranks(self):\n",
    "        \n",
    "        metric_rank_df = pd.DataFrame(range(0,96), columns=['rank'])\n",
    "        #mappings frorm pat\n",
    "        cell_maps = pd.read_csv('./data/TM_clusters_map.csv')\n",
    "        cell_maps.set_index('cluster', inplace=True)\n",
    "        self.idx_true = self.dfs_all[list(self.dfs_all.keys())[0]].index\n",
    "        for key, df in self.dfs_all.items():\n",
    "#             df = pd.read_csv(self.savepath+file, index_col = 0)\n",
    "            \n",
    "            df.columns = df.index = df.columns.str.replace(';|-', '.') \n",
    "            df.columns = df.index = cell_maps.loc[df.columns, 'map'] \n",
    "            ranks_all = df.T.reset_index().drop_duplicates().set_index('map').apply(lambda x: \n",
    "                                                        x.sort_values(ascending = False).index, \n",
    "                                                        axis = 0).reset_index(drop = True).apply(lambda x:\n",
    "                                                        x[x.name == x].index.to_list())\n",
    "            cluster_ranks = ranks_all.apply(lambda x:x[0])\n",
    "            metric_rank_df[key] = metric_rank_df.loc[:,'rank'].apply(lambda x: \n",
    "                                                                               (cluster_ranks < x).sum()/142)\n",
    "\n",
    "\n",
    "        metric_rank_df['rank'] = metric_rank_df['rank']/95\n",
    "        var_name_ = 'Measure_Top N genes'\n",
    "        self.plot_df = metric_rank_df.melt(id_vars = 'rank', \n",
    "                                           var_name = var_name_,\n",
    "                                           value_name='n_top_rank_clusters')\n",
    "        \n",
    "        self.plot_df['norm'] = list(map(lambda x: x[0], self.plot_df[var_name_].str.split('_')))\n",
    "        self.plot_df['n_top_genes'] = list(map(lambda x: str(x[1]), self.plot_df[var_name_].str.split('_')))\n",
    "        self.plot_df['n_top_genes'] = self.plot_df.n_top_genes.astype('category')\n",
    "        \n",
    "        auc_ = self.plot_df.groupby(['norm', \n",
    "                                'n_top_genes']).apply(lambda x: \n",
    "                                    auc(x['rank'], x['n_top_rank_clusters'])).to_frame(name = 'auc').reset_index()\n",
    "\n",
    "        self.plot_df = pd.merge(self.plot_df, auc_, how = 'left', on = ['norm', 'n_top_genes'])\n",
    "        self.plot_df[var_name_ + ' | AUC'] = self.plot_df[var_name_] +  ' | ' + self.plot_df['auc'].round(3).astype(str)\n",
    "    \n",
    "            \n",
    "#     files = os.listdir(self.savepath)\n",
    "    #investigating low perfonming clusters\n",
    "    def get_ranks_for_df(self, df):\n",
    "#         df = pd.read_csv(filename, index_col = 0)\n",
    "        idx_true = self.idx_true\n",
    "        idx_true.name = 'cluster'\n",
    "#         df.columns = df.index = df.columns.str.replace(';|-', '.') \n",
    "#         df.columns = df.index = cell_maps.loc[df.columns, 'map'] \n",
    "\n",
    "        ranks_all = df.T.reset_index().drop_duplicates().set_index('map').apply(lambda x: \n",
    "                                                                              x.sort_values(ascending = False).index, \n",
    "                                     axis = 0).reset_index(drop = True).apply(lambda x:\n",
    "                                                                              x[x.name == x].index.to_list())\n",
    "\n",
    "        ranks_all = ranks_all.apply(lambda x: x[0]).to_frame(name = 'rank')\n",
    "        ranks_all.index = idx_true\n",
    "        \n",
    "        return ranks_all, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score_normalized(data, use_score = 'localScore'):\n",
    "    data['Score'] = [d[use_score] for d in data.localScoreCellType]\n",
    "    data['cluster'] = data.cellType.str.replace(' ', '_') + ';'+data.tissueType.str.replace(' ', '_')\n",
    "    df = data.pivot(index = 'geneName', columns = 'cluster', values = 'Score')\n",
    "    normalized_df = (df-df.mean())/df.std()\n",
    "    gene_cluster_score = pd.DataFrame(normalized_df,\n",
    "                                columns=df.columns, \n",
    "                                index = df.index)\n",
    "    return gene_cluster_score, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_cluster_local_score_norm, gene_cluster_local_score = get_score_normalized(data_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "c_local = Literature_model(data = data_all,score_mat_true = gene_cluster_local_score,\n",
    "                           score_mat_normalized=gene_cluster_local_score_norm, use_score='localScore')\n",
    "c_local.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_cluster_global_score_norm, gene_cluster_global_score = get_score_normalized(data_all, use_score = 'globalScore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "c_global = Literature_model(data = data_all,score_mat_true = gene_cluster_global_score,\n",
    "                           score_mat_normalized=gene_cluster_global_score_norm, use_score='localScore')\n",
    "c_global.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### try to fit a model to see how different norms are performing? idk.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ranks_for_df(df, true_idx):\n",
    "#         df = pd.read_csv(filename, index_col = 0)\n",
    "    idx_true = true_idx\n",
    "    idx_true.name = 'cluster'\n",
    "#         df.columns = df.index = df.columns.str.replace(';|-', '.') \n",
    "#         df.columns = df.index = cell_maps.loc[df.columns, 'map'] \n",
    "\n",
    "    ranks_all = df.T.reset_index().drop_duplicates().set_index('map').apply(lambda x: \n",
    "                                                                          x.sort_values(ascending = False).index, \n",
    "                                 axis = 0).reset_index(drop = True).apply(lambda x:\n",
    "                                                                          x[x.name == x].index.to_list())\n",
    "\n",
    "    ranks_all = ranks_all.apply(lambda x: x[0]).to_frame(name = 'rank')\n",
    "    ranks_all.index = idx_true\n",
    "\n",
    "    return ranks_all, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_ranks(dfs, true_idx):\n",
    "    ranks_all = pd.DataFrame(0, columns = dfs.keys(), index = true_idx)\n",
    "    for key, df in dfs.items():\n",
    "        ranks_all.loc[:, key], _ = get_ranks_for_df(df, true_idx)\n",
    "        \n",
    "    return ranks_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks_all_global = get_all_ranks(c_global.dfs_all, c_global.idx_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks_all_global.to_csv('./data/rank_all_norms_by_globalScore.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_maps = pd.read_csv('./data/TM_clusters_map.csv')\n",
    "cell_maps.set_index('cluster', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ranks(dfs_all, score_type, ranks = False):\n",
    "    metric_rank_df = pd.DataFrame(range(0,96), columns=['rank'])\n",
    "    for key, df in dfs_all.items():\n",
    "        ranks_all = df.T.reset_index().drop_duplicates().set_index('map').apply(lambda x: \n",
    "                                                    x.sort_values(ascending = False).index, \n",
    "                                                    axis = 0).reset_index(drop = True).apply(lambda x:\n",
    "                                                    x[x.name == x].index.to_list())\n",
    "        cluster_ranks = ranks_all.apply(lambda x:x[0]).to_frame(name = 'rank')\n",
    "        if ranks:\n",
    "            cluster_ranks['Measure_top N genes'] = key\n",
    "            cluster_ranks['score'] = score_type\n",
    "            cluster_ranks.reset_index(inplace = True)\n",
    "            return cluster_ranks\n",
    "        metric_rank_df[key] = metric_rank_df.loc[:,'rank'].apply(lambda x: \n",
    "                                        (cluster_ranks < x).sum()/142)\n",
    "        \n",
    "        metric_rank_df['rank'] = metric_rank_df['rank']/95\n",
    "        var_name_ = 'Measure_Top N genes'\n",
    "        plot_df = metric_rank_df.melt(id_vars = 'rank', \n",
    "                                           var_name = var_name_,\n",
    "                                           value_name='n_top_rank_clusters')\n",
    "        \n",
    "        plot_df['norm'] = list(map(lambda x: x[0], plot_df[var_name_].str.split('_')))\n",
    "        plot_df['n_top_genes'] = list(map(lambda x: str(x[1]), plot_df[var_name_].str.split('_')))\n",
    "        plot_df['n_top_genes'] = plot_df.n_top_genes.astype('category')\n",
    "        \n",
    "        auc_ = plot_df.groupby(['norm', \n",
    "                                'n_top_genes']).apply(lambda x: \n",
    "                                    auc(x['rank'], x['n_top_rank_clusters'])).to_frame(name = 'auc').reset_index()\n",
    "\n",
    "        plot_df = pd.merge(plot_df, auc_, how = 'left', on = ['norm', 'n_top_genes'])\n",
    "        plot_df[var_name_ + ' | AUC'] = plot_df[var_name_] +  ' | ' + plot_df['auc'].round(3).astype(str)\n",
    "    \n",
    "    return plot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_local = pd.DataFrame(normalize(c_local.dfs_all['L1_25'], axis = 1), \n",
    "                          columns= c_local.dfs_all['L1_25'].columns,\n",
    "                          index = c_local.dfs_all['L1_25'].index)\n",
    "\n",
    "norm_global = pd.DataFrame(normalize(c_global.dfs_all['L1_25'], axis = 1), \n",
    "                          columns= c_global.dfs_all['L1_25'].columns,\n",
    "                          index = c_global.dfs_all['L1_25'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_all = (0.0*c_local.dfs_all['L1_25'] + 1.0*c_global.dfs_all['L1_25']).copy(deep = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks_norm_sum = get_ranks({'L1_25': norm_all}, 'norm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_global.plot_df[c_global.plot_df['Measure_Top N genes'] == 'L1_25']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks_norm_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_local.dfs_all['L1_05'].T.reset_index().drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks_norm_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks_norm_sum['map'] = c_local.gene_cluster_score.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks_norm_sum.sort_values(by = 'rank', ascending = False).to_csv('./data/ranks_normalized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks_local = get_ranks(norm_global, 'local')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks_global = get_ranks(c_global.dfs_all, 'global')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = ranks_local.reset_index().melt(id_vars = ['map', 'Measure_top N genes', 'score'],\n",
    "                                   value_vars = 'rank', value_name = 'rank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks_all = pd.merge(ranks_local, ranks_global, on = ['map', \n",
    "                                                'Measure_top N genes'], suffixes=['_local', '_global'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks_all['Measure_top N genes'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = px.scatter(ranks_all.loc[ranks_all['Measure_top N genes'] == 'L1_25'],\n",
    "                 x='rank_local', y='rank_global', hover_data=['map'])\n",
    "fig.update_traces(textposition='top center')\n",
    "fig.update_layout(height = 800,\n",
    "    title_text='Literature ranks loval vs global'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.write_html('./plots/lit_local_vs_global_ranks.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks_norm_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "var_name_ =  'Measure_Top N genes | AUC'\n",
    "fig = plt.figure(figsize=(14,10))\n",
    "\n",
    "ax = sns.lineplot(data = ranks_norm_sum, \n",
    "             x = 'rank', \n",
    "             y = 'n_top_rank_clusters')\n",
    "#              hue=var_name_, hue_order=sorted(ranks_norm_sum[var_name_].unique()),\n",
    "#              palette=sns.color_palette(\"Paired\", 15))\n",
    "# plt.setp(ax.get_legend().get_texts(), fontsize='15') # for legend text\n",
    "# plt.setp(ax.get_legend().get_title(), fontsize='20')\n",
    "ax.axes.set_title(\"Literature Based Cluster Predictions (Tabula Muris; Global Score)\",fontsize=25)\n",
    "ax.set_xlabel(\"Rank Threshold\",fontsize=20)\n",
    "ax.set_ylabel(\"Fractions of Clusters Labeled Correctly\",fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "def plot_auc(df, score):\n",
    "    var_name_ =  'Measure_Top N genes | AUC'\n",
    "    fig = plt.figure(figsize=(14,10))\n",
    "\n",
    "    ax = sns.lineplot(data = df, \n",
    "                 x = 'rank', \n",
    "                 y = 'n_top_rank_clusters',\n",
    "                 hue=var_name_, hue_order=sorted(df[var_name_].unique()),\n",
    "                 palette=sns.color_palette(\"Paired\", 12))\n",
    "    plt.setp(ax.get_legend().get_texts(), fontsize='15') # for legend text\n",
    "    plt.setp(ax.get_legend().get_title(), fontsize='20')\n",
    "    ax.axes.set_title(\"Literature Based Cluster Predictions (Tabula Muris; %s Score)\"%score,fontsize=25)\n",
    "    ax.set_xlabel(\"Rank Threshold\",fontsize=20)\n",
    "    ax.set_ylabel(\"Fractions of Clusters Labeled Correctly\",fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_auc(c_local.plot_df, score = 'Local')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig('./plots/lit_prredictions_auc.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_auc(c_global.plot_df, score = 'Global')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### some exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_mean_vectors = data_all.groupby('cluster').apply(lambda x: get_top_genes_vector(x, top_n = 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_mean_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lc.columns = lc.columns.str.replace(';|-', '.') \n",
    "\n",
    "# rank_df.to_csv('./data/ranks_l1_25_global_score.csv')\n",
    "\n",
    "# l = df.T.reset_index().drop_duplicates().set_index('map').apply(lambda x: \n",
    "#                                                                           x.sort_values(ascending = False).index, \n",
    "#                                  axis = 0).reset_index(drop = True).iloc[0:9, -1].to_list()\n",
    "\n",
    "# df.T.reset_index().drop_duplicates().set_index('map').apply(lambda x: \n",
    "#                                                                           x.sort_values(ascending = False).index, \n",
    "#                                  axis = 0).reset_index(drop = True)\n",
    "\n",
    "# cell_maps.loc[cell_maps['map'].isin(l)]\n",
    "# lc = pd.read_csv('./data/local_score_top_25_type_II_pneumocytes.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preevious "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_mean_vectors.loc[slice('Alveolar_macrophages;lung')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection =  'patrick-cells-pl-200304'\n",
    "n = 10\n",
    "def query_lab(gene_list, n=n, collection=collection, window = 101):\n",
    "    result_df = pd.DataFrame()\n",
    "    for gene in tqdm(gene_list[0:n]): \n",
    "        query = gene\n",
    "        res = nf.get_signals_lab(query = query, control_collection = collection, \n",
    "                                 window = window)\n",
    "\n",
    "        if type(res)!= str:\n",
    "            res = pd.DataFrame(res)\n",
    "            res['gene'] = gene\n",
    "            result_df = pd.concat([result_df, res])\n",
    "        #res.token = res.token.str.upper()\n",
    "     \n",
    "    return result_df.pivot(index = 'gene', columns = 'token', values = 'score').astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = top_genes.iloc[0:1].apply(query_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_all = top_genes.apply(query_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syns_all  =  set([tok for ls in syns for tok in ls])\n",
    "def sort_df(df, q = [0.3, 0.5, 0.75], by = [0.75, 0.5, 0.3]):\n",
    "    df_collapsed = pd.DataFrame()\n",
    "    df = df.T\n",
    "    no_match = df.index.difference(syns_all)\n",
    "#     print(no_match)\n",
    "    df.insert(loc = 0, column = 'count', value = n - df.isna().sum(1))\n",
    "    for i, syn in tqdm(enumerate(syns)):\n",
    "        df_subset = df[df.index.isin(syn)]\n",
    "        if not df_subset.empty:\n",
    "#             print(i)\n",
    "#             print('\\n',df_subset.index)\n",
    "            \n",
    "            df_idx = df_subset['count'].idxmax()\n",
    "            df_max = df_subset.max()\n",
    "            df_max.name = df_idx\n",
    "            df_collapsed = pd.concat([df_collapsed, df_max], axis=1)\n",
    "    if not df.loc[no_match,:].empty:\n",
    "        df_collapsed = pd.concat([df_collapsed, df.loc[no_match,:].T], axis = 1)\n",
    "    quantiles = df_collapsed.quantile(q, axis = 0)\n",
    "    df_collapsed = pd.concat([quantiles, df_collapsed]).T\n",
    "    df_collapsed = df_collapsed.sort_values(by = by, ascending = False)\n",
    "    \n",
    "    return df_collapsed.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[0].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dfs = list(map(sort_df, df_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = df_all[0].columns.difference(syns_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[0].loc[:,'yolk_sac_derived_macrophages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lit_df = pd.DataFrame()\n",
    "for i, idx in enumerate(top_genes.index):\n",
    "    df = sorted_dfs[i].copy(deep = True)\n",
    "#     df.insert(loc = 0, column = 'count', value = n - df.isna().sum(1))\n",
    "    df.insert(loc = 0, column = 'tissue', value = idx[0])\n",
    "    df.insert(loc = 0, column = 'true_cell', value = idx[1])\n",
    "    df = df.reset_index(drop = False, inplace = False)\n",
    "    df.columns = ['token'] + df.columns[1:].to_list()\n",
    "#     df = df[df['count'] > 3]\n",
    "    lit_df = pd.concat([lit_df, df.loc[0:10, ['token', 'true_cell', 'tissue', 'count']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_df.to_csv('./data/t10_genes_per_cluster.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ees_tokens = pd.read_csv('./data/appendTokens.csv', sep = '\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ees_tokens.columns = ['token', 'syn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = ees_tokens.groupby('token').apply(lambda x: x.syn.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_syns = [set([idx]+syn) for idx, syn in synonyms.iteritems()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = sorted_dfs[0].index\n",
    "syn_mat = pd.DataFrame(np.eye(len(tokens)), index=idx, columns=idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ees_ = ees_tokens.copy(deep = True)\n",
    "ees_['val'] = 1\n",
    "ees_1 = pd.pivot_table(ees_, index='token', columns='syn', values = 'val').fillna(0)\n",
    "ees_2 = pd.pivot_table(ees_, index='syn', columns='token', values = 'val').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ees_1.index.intersection(ees_2.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_syns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = {i: [] for i in range(len(all_syns))}\n",
    "duplicates = {i: [] for i in range(len(all_syns))}\n",
    "for i, syn_set in enumerate(all_syns):\n",
    "    for j, syn_set_ in enumerate(all_syns):\n",
    "        if (i!=j) & (len(syn_set.intersection(syn_set_)) > 0):\n",
    "            print('\\n',i, j, '\\nIntersection: ', syn_set.intersection(syn_set_))\n",
    "            if syn_set_ == syn_set:\n",
    "                duplicates[i].append(j)\n",
    "            else:\n",
    "                subset[i].append(j)\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syns = []\n",
    "for k, v in subset.items():\n",
    "    if v:\n",
    "        syn_subset = synonyms.iloc[[k]+v]\n",
    "        print(syn_subset.index)\n",
    "        s = list(set(syn_subset.explode().to_list() + syn_subset.index.to_list()))\n",
    "        if s not in syns:\n",
    "            syns.append(s)\n",
    "    else:\n",
    "        syns.append(list(set(synonyms.iloc[k] + [synonyms.index[k]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "syns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[d.index.isin(syns[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
